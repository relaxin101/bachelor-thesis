% Copyright (C) 2014-2024 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Alexander Rinsche} % The author name without titles.
\newcommand{\thesistitle}{Context-sensitive Semantic Search in Graphs} % The title of the thesis. The English version should be used, if it exists.

% Create the XMP metadata file for the creation of PDF/A compatible documents.
\begin{filecontents*}[overwrite]{\jobname.xmpdata}
\Author{\authorname}                                    % The author's name in the document properties.
\Title{\thesistitle}                                    % The document's title in the document properties.
\Language{de-AT}                                        % The document's language in the document properties. Select 'en-US', 'en-GB', or 'de-AT'.
\Keywords{a\sep list\sep of\sep keywords}               % The document's keywords in the document properties (separated by '\sep ').
\Publisher{TU Wien}                                     % The document's publisher in the document properties.
\Subject{Thesis}                                        % The document's subject in the document properties.
\end{filecontents*}

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesetting of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}        % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes}  % Provides tooltip-like todo notes.
\usepackage{morewrites} % Increases the number of external files that can be used.
\usepackage[a-2b,mathxmp]{pdfx}      % Enables PDF/A compliance. Loads the package hyperref and has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists of acronyms. This package has to be included last.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around hyperlinks (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph indentation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setadvisor{Prof.}{Emanuel Sallinger}{}{male}

% For bachelor and master theses:
\setfirstassistant{Dr.}{Eleonora Laurenza}{}{female}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{12120519}
\setdate{01}{01}{2025} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{Kontext-sensitive Semantische Suche in Graphen} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.

% For bachelor and master:
\setcurriculum{Software \& Information Engineering}{Software \& Information Engineering} % Sets the English and German name of the curriculum.

% Optional reviewer data:
\setfirstreviewerdata{Affiliation, Country}
\setsecondreviewerdata{Affiliation, Country}


\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

\addtitlepage{naustrian} % German title page.
\addtitlepage{english} % English title page.
\addstatementpage

\begin{danksagung*}
\todo{Ihr Text hier.}
\end{danksagung*}

\begin{acknowledgements*}
\todo{Enter your text here.}
\end{acknowledgements*}

\begin{kurzfassung}
\todo{Ihr Text hier.}
\end{kurzfassung}

\begin{abstract}
\todo{Enter your text here.}
\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}
What is the definition of Information? To further specify the question, such a definition should cover the following commonly observed properties of information:
\begin{itemize}
    \item The same sentence contains a different amount of information depending on the receiver and their context
    \item It's verb ''to inform'' means to convey a fact that should influence someone's behavior.
    \item An ''informed decision'' means to be aware of as many relevant facts as possible concerning a certain decision.
\end{itemize} 
By those properties, the definition of information for this thesis will be ''facts or knowledge relevant to a receiver that influence their behavior or decisions''. The amount of information can be determined by the severity of change in their actions depending on getting or missing a said piece of information. For a machine learning model, this can essentially be measured by loss. A data sample with a high loss obviously contains more information than the last ten data samples that the model predicted correctly and therefore had little to no loss. 

From this there is a clear distinction between a Knowledge Base and a Information System: While a Knowledge Base is a collection of facts or, more generally speaking, knowledge, an Information System needs to actively filter the facts relevant to a user or a query. 
%So although a database can be considered a perfectly suitable Information System to experienced developers, for a common user there is too much (to them) uninteresting information and overhead attached to properly use it. 
For a Knowledge Base to become an Information System, a mechanism that produces the relevant information to their users at the moment where it becomes relevant, without additional overhead of filtering out unnecessary data, is paramount. In this thesis we will explore and evaluate one possible mechanism leveraging Text Embeddings and Knowledge Graph Technology.

\section{Motivation}
The recent success of transformer architectures has affected most areas of Computer Science to some degree. In the field of Information Retrieval and Search through document corpi, the technique of Semantic Search has gained new traction. While previous approaches like one hot encodings or continuous bag of words all had their shortcomings, new transformer architectures are able to capture complex relationships and meanings of words in dense vectors and through the attention mechanism that the transformer architectures all share even ambiguities of words are representable in these models.
The price is the high initial cost to train these models, the billions of parameters that need to be loaded in active memory and additional finetuning that is often required when it comes to more specialized domains.
The computational resources required for these tasks are still infeasible to most individuals and even many companies, which is why big tech companies like Google and OpenAI are currently so successful. The aim of this thesis is to propose an alternative architecture where semantic embeddings don't have to be perfect but can be improved later on without fine-tuning of the embedding model itself and rather through exploiting additional information about the domain. The proposed approach should also produce a decent recommendation system and maybe even a semantic search that can take user-based relevance into account. To achieve this, the problem won't be a loose collection of documents, but a set of documents and users organized and linked in a rich taxonomy. By leveraging the information encoded in a neighborhood in such a graph, additional information about the context should be includable through basic Graph Neural Networks (GNNs).


\section{Data Sources}
The necessary data for training and evaluating the model was obtained from qibri.
qibri is a Knowhow Management System targeted at medium to large companies with highly specialized domain-specific knowledge. Its purpose is to manage and unify all documentation of Knowhow in a central place, to create a better overview of how well the core processes and products are documented and provide the necessary documentation to an employee when they needed. It has several mechanisms to encourage users to regularly check and update the Knowhow that they are responsible for, a versioning and approval process system to make sure changes are transparent, can be backtracked and are formally accepted by decision makers. Knowledge retrieval is particularly difficult in this context, however - there's a lot of information that might seem relevant when searching for a specific subject, but usually a lot of it just isn't relevant to the user who started the search. So, although a document might seem to contain a lot of keywords relevant to a given search, it might still be the case that it just isn't relevant due to it's particular scope and target audience within a company. To mitigate this, all documents and users are organized in a taxonomy, containing locations, departments, relevant topics, etc. Using this taxonomy, one can quickly get an idea which documents are more closely related to a user's daily work and which might be less relevant, just by using the distances between them. Unifying a semantic search with the relevance scoring particular to the user triggering the search will be another area of exploration of this thesis, although the necessary data to evaluate such a joint search is currently not available and will only be obtainable through a feedback loop.

\section{Problem Statement}
Finding, optimizing and verifying such a system unfortunately exceeded the scope of a bachelor thesis. Therefore, the focus of this work will be on creating a sensible framework for quickly iterating over different versions. The main challenges this thesis posed were
\begin{description}

    \item[history preservation] i.e. incorporating some kind of versioning so that different extraction runs can easily be compared to each other.
    \item[extendability] building the pipeline in a way such that changes to the data extraction and the embedding process are ''easy'' to implement.
    \item[ease of deployment] - due to the classic difficulties of evaluating a recommender system or a search engine (difficulties defining ''correct'' results, relevances for different people, etc.), one of the main requirements imposed by qibri was the option to quickly deploy to production and to test it there.
\end{description}
In addition to that, there are three iterations of the system that are going to be explicitly compared: A naive Semantic Search, A static Message Passing Contextualization, where no parameters are actually learned and context is statically aggregated and a proper GNN-based approach, where actual learnable parameters are fit. These steps should give a decent first idea if the proposed methodology seems promising or not.


\chapter{Methodology}
The main technologies used for this thesis were python with several important libraries, to provide the necessary functionality, and docker, for ease of development and deployment. The main packages used were pytorch and pytorch\_geometric, for the GNN training, sentence transformers, which is a package provided by Huggingface, an online hub for publicizing open source models and was used for obtaining the initial embedding model, and fastAPI, which is a python libary meant to quickly set up a webserver.
A smaller libary, that deserves to be mentioned specially, is logica. logica is a python package meant to transpile a datalog dialect (a successor of Yedalog \ref{something}) into native SQL. This was used for the logical filtering
\section{Architecture and Code}
The search is built as a standalone docker container with all secrets passed via an env file. The results are written to an external volume specified in the docker-compose file, where an sqlite3 file contains the knowledge graph and all node embeddings, while a subfolder contains all files downloaded from the qibri backend and, where possible, an additional file containing the extracted content using Chunkr \footnote{An external service for extracting text from a variety of files \url{https://chunkr.ai/}}. The sqlite3 database is checked into a separate git repository to make it possible to go to a specific run of the pipeline and compare results.

There are two different entry points for the program. One is a webserver, to provide the actual search as a microservice. The second is the pipeline, which starts a fresh extraction of the Knowledge Graph of a specific customer from qibri, writes that to the SQLite file (after dropping it), downloads potentially missing files, runs the GNN training loop and finally writes the obtained embeddings to the SQLite file as well, thereby providing the vectors used by the search in the webserver.

\section{Text Embeddings}
Representing words or, more generally speaking, text, as (relatively) low-dimensional embedding vectors (instead of the trivial method of one-hot-encoding a dictionary) such that relative distances and orientation capture some notion of meaning is one of if not the main topic of Natural Language Processing (NLP). One of the first big successes on this field was Word2Vec, which was published in a paper by Mikolov et al., at Google, in 2013. This technique uses a sliding window approach where every word in the vocabulary of the model gets projected to a dense vector and these vectors are then fit using one of two methods: \textit{Continuous bag of words} or \textit{continuous skip-gram}. Both of these methods use simple linear transformations where the one-hot-encoded word vector gets projected to a dense vector space with dimensionality $\mathcal{D}$, while the bag-of-words approach uses the surrounding to predict a missing word, the skip-gram version uses a specific input word to predict the surrounding. The novel part of these approaches is that the predicitons are computed by another simple linear layer followed by a softmax, without any non-linearities or any larger depth. \cite{word2vec_preprint} This however suffices to learn relationships like $v('France') - v('Paris') + v('Madrid') \approx v('Spain')$, while still being quite efficient to compute. \cite{word2vec_official} The limitations arise from more complex relationships and a deeper understanding of text. By construction, Word2Vec has no mechanism to alter meaning by context, it doesn't even capture the sequence of words. As the term "Bag of Words" implies, the model considers no notion of ordering, words that appear before and after are summed up, averaged, and then the most similar word is predicted. The skip-gram approach also only predicts words that are also likely to appear given a specific input word, not the \textit{next} or \textit{previous} words. In addition to that, words that have several different meanings like ''bear'' or ''bow'' also aren't distinguished. The semantic nuances are accumulated in a single embedding vector and any predictions are ambiguous if they were a result of the verb or the noun. A final drawback is that words that weren't included in the initial vocabulary can't be assigned an embedding later on, word2vec can't assign new embeddings on the fly. With these strengths and drawbacks in mind, let's consider a more recent alternative. 


% Bert
The paper \textit{Attention is all you need} \cite{vaswani_attention_2017} by Vaswani et al. first describes the Transformer architecture, now prominent in Large Language Models. In contrast to Word2Vec, embeddings in this architecture consist of the vector associated with the token according to the vocabulary plus a positional encoding (a combination of sine and cosine functions depending distinct for every dimension of the embedding). Additionally, although the training process bears some resemblance to the CBOW variant of Word2Vec, there are some major improvements. Instead of just predicting \textbf{a} word in a sentence based on an unordered bag consisting of previous and following words, the training process used a sequence of words as an input and the task was to predict the next word. The ''byproduct'' of this process is essentially one of the earlier versions of ChatGPT - a model that predicts what the next word (or rather token, which might also be something like a syllable) should be in a conversation repeatedly. The internal architecture differs heavily from the initial Word2Vec model. Several blocks

% GPTs and derivatives

\chapter{Results}

% Remove following line for the final thesis.
%\input{intro.tex} % A short introduction to LaTeX.

\chapter{Discussion}


\backmatter

% Declare the use of AI tools as mentioned in the statement of originality.
% Use either the English aitools or the German kitools.
\begin{aitools}
\todo{Ihr Text hier.}
\end{aitools}

\begin{kitools}
\todo{Enter your text here.}
\end{kitools}

% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
\printindex

% Add a glossary.
\printglossaries

% Add a bibliography.
\bibliographystyle{alpha}
\bibliography{intro}

\end{document}